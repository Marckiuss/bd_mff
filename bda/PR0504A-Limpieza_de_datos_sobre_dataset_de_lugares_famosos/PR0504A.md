## PR0504A. Limpieza de datos sobre dataset de lugares famosos
Seguimos trabajando con dataframes en PySpark. En esta ocasión el objetivo es transformar datos crudos de destinos turísticos para limpieza de texto, cálculos matemáticos avanzados y gestión de fechas.

Supón que en la empresa en la que estás trabajando está preparando un catálogo turístico y el departamento de marketing necesita un dataset enriquecido con códigos cortos para la app móvil, precios ajustados psicológicamente y fechas límite para ofertas promocionales.

Trabajarás sobre el mismo archivo del dataset de Lugares Famosos del Mundo


```python
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType
from pyspark.sql.functions import col, lit, log
from pyspark.sql import functions as F

# Crear la sesión de Spark
spark = SparkSession.builder \
    .appName("PR0504 Limpieza de datos sobre dataset de lugares famosos") \
    .getOrCreate()
```

    Setting default log level to "WARN".
    To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
    26/02/24 10:11:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable



```python
from pyspark.sql.types import StructType, StructField, StringType, DoubleType
from pyspark.sql.functions import col, lit

# Lugares famosos del mundo
schema_lugares = StructType([
    StructField("Place_Name", StringType(), True),
    StructField("Country", StringType(), True),
    StructField("City", StringType(), True),
    StructField("Annual_Visitors_Millions", DoubleType(), True),
    StructField("Type", StringType(), True),
    StructField("UNESCO_World_Heritage", StringType(), True),
    StructField("Year_Built", StringType(), True),
    StructField("Entry_Fee_USD", DoubleType(), True),
    StructField("Best_Visit_Month", StringType(), True),
    StructField("Region", StringType(), True),
    StructField("Tourism_Revenue_Million_USD", DoubleType(), True),
    StructField("Average_Visit_Duration_Hours", DoubleType(), True),
    StructField("Famous_For", StringType(), True)
])

df_lugares = spark.read.format("csv") \
    .option("header", "true") \
    .schema(schema_lugares) \
    .load("data/world_famous_places_2024.csv")

df_lugares.printSchema()
df_lugares.show(5)
```

    root
     |-- Place_Name: string (nullable = true)
     |-- Country: string (nullable = true)
     |-- City: string (nullable = true)
     |-- Annual_Visitors_Millions: double (nullable = true)
     |-- Type: string (nullable = true)
     |-- UNESCO_World_Heritage: string (nullable = true)
     |-- Year_Built: string (nullable = true)
     |-- Entry_Fee_USD: double (nullable = true)
     |-- Best_Visit_Month: string (nullable = true)
     |-- Region: string (nullable = true)
     |-- Tourism_Revenue_Million_USD: double (nullable = true)
     |-- Average_Visit_Duration_Hours: double (nullable = true)
     |-- Famous_For: string (nullable = true)
    
    +-------------------+-------------+----------------+------------------------+------------------+---------------------+----------------+-------------+-----------------+--------------+---------------------------+----------------------------+--------------------+
    |         Place_Name|      Country|            City|Annual_Visitors_Millions|              Type|UNESCO_World_Heritage|      Year_Built|Entry_Fee_USD| Best_Visit_Month|        Region|Tourism_Revenue_Million_USD|Average_Visit_Duration_Hours|          Famous_For|
    +-------------------+-------------+----------------+------------------------+------------------+---------------------+----------------+-------------+-----------------+--------------+---------------------------+----------------------------+--------------------+
    |       Eiffel Tower|       France|           Paris|                     7.0|    Monument/Tower|                   No|            1889|         35.0|May-June/Sept-Oct|Western Europe|                       95.0|                         2.5|Iconic iron latti...|
    |       Times Square|United States|   New York City|                    50.0|    Urban Landmark|                   No|            1904|          0.0|Apr-June/Sept-Nov| North America|                       70.0|                         1.5|Bright lights, Br...|
    |      Louvre Museum|       France|           Paris|                     8.7|            Museum|                  Yes|            1793|         22.0|        Oct-March|Western Europe|                      120.0|                         4.0|World's most visi...|
    |Great Wall of China|        China|Beijing/Multiple|                    10.0| Historic Monument|                  Yes|220 BC - 1644 AD|         10.0| Apr-May/Sept-Oct|     East Asia|                      180.0|                         4.0|Ancient defensive...|
    |          Taj Mahal|        India|            Agra|                     7.5|Monument/Mausoleum|                  Yes|            1653|         15.0|        Oct-March|    South Asia|                       65.0|                         2.0|White marble maus...|
    +-------------------+-------------+----------------+------------------------+------------------+---------------------+----------------+-------------+-----------------+--------------+---------------------------+----------------------------+--------------------+
    only showing top 5 rows
    


## Ejercicio 1: Generación de códigos SKUs

La App móvil no puede mostrar nombres largos. Necesitamos un **SKU (Stock Keeping Unit)** para cada lugar.

Para ello tienes que crear una columna **SKU_Lugar** en un nuevo DataFrame **df_feat**. El formato debe ser **PAIS(3)-CIUDAD(3)-TIPO**.


Debes tener en cuenta:

**País**: extrae los 3 primeros caracteres del Country y conviértelos a mayúsculas (upper, substring).

**Ciudad**: extrae los 3 primeros caracteres de City. Si la ciudad tiene menos de 3 letras (raro, pero posible), rellena con ‘X’ a la derecha (rpad).

**Tipo**: la columna Type a veces tiene barras (ej: “Monument/Tower”). Queremos solo la primera parte antes de la barra. Usa split para dividir el texto y extrae el primer elemento (índice 0).

**Unión**: concatena todo con guiones bajos (concat_ws).


```python
from pyspark.sql.functions import col, upper, lpad, split, concat_ws, lit, concat, substring

df_feat = ( df_lugares
            .withColumn(
            "SKU_Lugar",
            concat_ws("_",
            substring(col("Country"),1,3),
            lpad(substring(col("City"),1,3),3,"X"),
            split(col("Type"),"/")[0]
                     )
            )
)

df_feat.select("Country","City","Type","SKU_Lugar").show(5)
```

    +-------------+----------------+------------------+--------------------+
    |      Country|            City|              Type|           SKU_Lugar|
    +-------------+----------------+------------------+--------------------+
    |       France|           Paris|    Monument/Tower|    Fra_Par_Monument|
    |United States|   New York City|    Urban Landmark|Uni_New_Urban Lan...|
    |       France|           Paris|            Museum|      Fra_Par_Museum|
    |        China|Beijing/Multiple| Historic Monument|Chi_Bei_Historic ...|
    |        India|            Agra|Monument/Mausoleum|    Ind_Agr_Monument|
    +-------------+----------------+------------------+--------------------+
    only showing top 5 rows
    


## Ejercicio 2: Ajuste de precios y tiempos
Necesitamos normalizar las métricas para el algoritmo de recomendación.

Añade las siguientes columnas numéricas:

**Duracion_Techo**: la Average_Visit_Duration_Hours tiene decimales (2.5 horas). Redondea siempre hacia arriba (ceil) para reservar bloques completos en la agenda del turista.

**Log_Ingresos**: los ingresos (Tourism_Revenue_Million_USD) varían demasiado (de 45 a 180). Aplica una transformación logarítmica (log10) para suavizar la escala.

**Mejor_Oferta**: compara el Entry_Fee_USD actual contra un “Precio de Competencia” simulado (que es siempre 20 USD). Usa la función least para quedarte con el precio más bajo de los dos (fila a fila)


```python
from pyspark.sql.functions import col, upper, lpad, split, concat_ws, lit, concat, ceil, regexp

df_anadido = ( df_lugares
            .withColumn("Duracion_Techo",F.ceil(col("Average_Visit_Duration_Hours"))) \
            .withColumn("Log_Ingresos",F.log(col("Tourism_Revenue_Million_USD"))) \
            .withColumn("Mejor_Oferta",F.least("Entry_Fee_USD",lit("20").cast("int")))
)

# Visualizamos el resultado para confirmar el formato
df_anadido.select("Duracion_Techo","Log_Ingresos","Mejor_Oferta").show(5, truncate=False)
```

    +--------------+-----------------+------------+
    |Duracion_Techo|Log_Ingresos     |Mejor_Oferta|
    +--------------+-----------------+------------+
    |3             |4.553876891600541|20.0        |
    |2             |4.248495242049359|0.0         |
    |4             |4.787491742782046|20.0        |
    |4             |5.19295685089021 |10.0        |
    |2             |4.174387269895637|15.0        |
    +--------------+-----------------+------------+
    only showing top 5 rows
    


## Ejercicio 3: Limpieza de texto
La columna **Famous_For** es demasiado larga para las notificaciones push.

Haz lo siguiente:

**Crea Desc_Corta**: extrae solo los primeros 15 caracteres de **Famous_For** (substring).

**Crea Ciudad_Limpia**: reemplaza la cadena “**New York City**” por “**NYC**” usando **regexp_replace** en la columna **City**.


```python
df_limpio = ( df_anadido
                    .withColumn("Desc_Corta",substring(col("Famous_For"),1,15))
                    .withColumn("Ciudad_Limpia",F.regexp_replace("City","New York City","NYC"))
)

df_limpio.select("Desc_Corta","Ciudad_Limpia").show(5)
```

    +---------------+----------------+
    |     Desc_Corta|   Ciudad_Limpia|
    +---------------+----------------+
    |Iconic iron lat|           Paris|
    |Bright lights, |             NYC|
    |World's most vi|           Paris|
    |Ancient defensi|Beijing/Multiple|
    |White marble ma|            Agra|
    +---------------+----------------+
    only showing top 5 rows
    


## Ejercicio 4: Gestión de fechas de campaña
Vamos a simular que lanzamos una campaña hoy.

Crea una columna **Inicio_Campana** usando **to_date** con la fecha “**2024-06-01**”.

Crea **Fin_Campana**: Suma 90 días a la fecha de inicio (date_add).

Crea **Dias_Hasta_Fin**: Calcula la diferencia en días entre el fin de la campaña y la fecha de construcción del monumento.

*Nota*:
Como **Year_Built** es un número (ej. 1889), primero deberás crear una fecha ficticia de construcción. Usa **concat** para unir el año con “**-01-01**” (ej: “**1889-01-01**”) y conviértelo a fecha con **to_date**.

Si el año tiene texto (como “220 BC”), *to_date* devolverá **null**, lo cual es correcto para este ejercicio.
Queremos saber cuál fue el insumo químico más pesado aplicado en cada parcela. Crea una columna llamada **Max_Quimico_kg**.


```python
df_fechas = (df_limpio
            .withColumn("Inicio_Campana",F.to_date(F.concat(F.col("Year_Built"),F.lit("-01-01"))))
            .withColumn("Fin_Campana",F.date_add(col("Inicio_Campana"),90))
            .withColumn("Dias_Hasta_Fin",F.datediff(col("Fin_Campana"),col("Inicio_Campana")))
)

df_fechas.select("Inicio_Campana","Fin_Campana","Dias_Hasta_Fin").show(5)
```

    +--------------+-----------+--------------+
    |Inicio_Campana|Fin_Campana|Dias_Hasta_Fin|
    +--------------+-----------+--------------+
    |    1889-01-01| 1889-04-01|            90|
    |    1904-01-01| 1904-03-31|            90|
    |    1793-01-01| 1793-04-01|            90|
    |          NULL|       NULL|          NULL|
    |    1653-01-01| 1653-04-01|            90|
    +--------------+-----------+--------------+
    only showing top 5 rows
    


    26/02/24 10:12:04 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors

