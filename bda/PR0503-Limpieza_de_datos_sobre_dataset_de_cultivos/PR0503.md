## PR0503. Limpieza de datos sobre dataset de cultivos
Seguimos avanzando en el conocimiento de PySpark realizando tareas más avanzadas.

## Dataset 1: Datos para la predicción del rendimiento en cultivos
Supón que queremos preparar los datos de nuestro dataset de cultivos para un modelo de redes neuronales y nos han pedido cuatro transformaciones específicas:

Generar identificadores únicos estandarizados
Normalizar las distribuciones numéricas
Comparar insumos
Proyectar fechas de cosecha


```python
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType
from pyspark.sql.functions import col, lit, log
from pyspark.sql import functions as F

# Crear la sesión de Spark
spark = SparkSession.builder \
    .appName("PR0503 Limpieza de datos sobre dataset cultivos") \
    .getOrCreate()
```

    Setting default log level to "WARN".
    To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
    26/01/28 12:18:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable



```python
from pyspark.sql.types import StructType, StructField, StringType, DoubleType
from pyspark.sql.functions import col, lit

# Datos para la predicción del rendimiento en cultivos
schema_cultivos = StructType([
    StructField("cultivo", StringType(), True),
    StructField("region", StringType(), True),
    StructField("tipo_suelo", StringType(), True),
    StructField("ph", DoubleType(), True),
    StructField("lluvia_mm", DoubleType(), True),
    StructField("temp_c", DoubleType(), True),
    StructField("humedad_pct", DoubleType(), True),
    StructField("abono_kg", DoubleType(), True),
    StructField("sistema_riego", StringType(), True),
    StructField("pesticida_kg", DoubleType(), True),
    StructField("densidad", DoubleType(), True),
    StructField("pre_cultivo", StringType(), True),
    StructField("ton_per_ha", DoubleType(), True)
])

df_cultivos = spark.read.format("csv") \
    .option("header", "true") \
    .schema(schema_cultivos) \
    .load("data/crop_yield_dataset.csv")

df_cultivos.printSchema()
df_cultivos.show(5)
```

    root
     |-- cultivo: string (nullable = true)
     |-- region: string (nullable = true)
     |-- tipo_suelo: string (nullable = true)
     |-- ph: double (nullable = true)
     |-- lluvia_mm: double (nullable = true)
     |-- temp_c: double (nullable = true)
     |-- humedad_pct: double (nullable = true)
     |-- abono_kg: double (nullable = true)
     |-- sistema_riego: string (nullable = true)
     |-- pesticida_kg: double (nullable = true)
     |-- densidad: double (nullable = true)
     |-- pre_cultivo: string (nullable = true)
     |-- ton_per_ha: double (nullable = true)
    


    26/01/28 12:18:15 WARN CSVHeaderChecker: CSV header does not conform to the schema.
     Header: Crop, Region, Soil_Type, Soil_pH, Rainfall_mm, Temperature_C, Humidity_pct, Fertilizer_Used_kg, Irrigation, Pesticides_Used_kg, Planting_Density, Previous_Crop, Yield_ton_per_ha
     Schema: cultivo, region, tipo_suelo, ph, lluvia_mm, temp_c, humedad_pct, abono_kg, sistema_riego, pesticida_kg, densidad, pre_cultivo, ton_per_ha
    Expected: cultivo but found: Crop
    CSV file: file:///workspace/PR0503-Limpieza_de_datos_sobre_dataset_de_cultivos/data/crop_yield_dataset.csv


    +-------+--------+----------+----+---------+------+-----------+--------+-------------+------------+--------+-----------+----------+
    |cultivo|  region|tipo_suelo|  ph|lluvia_mm|temp_c|humedad_pct|abono_kg|sistema_riego|pesticida_kg|densidad|pre_cultivo|ton_per_ha|
    +-------+--------+----------+----+---------+------+-----------+--------+-------------+------------+--------+-----------+----------+
    |  Maize|Region_C|     Sandy|7.01|   1485.4|  19.7|       40.3|   105.1|         Drip|        10.2|    23.2|       Rice|    101.48|
    | Barley|Region_D|      Loam|5.79|    399.4|  29.1|       55.4|   221.8|    Sprinkler|        35.5|     7.4|     Barley|    127.39|
    |   Rice|Region_C|      Clay|7.24|    980.9|  30.5|       74.4|    61.2|    Sprinkler|        40.0|     5.1|      Wheat|     68.99|
    |  Maize|Region_D|      Loam|6.79|   1054.3|  26.4|       62.0|   257.8|         Drip|        42.7|    23.7|       None|    169.06|
    |  Maize|Region_D|     Sandy|5.96|    744.6|  20.4|       70.9|   195.8|         Drip|        25.5|    15.6|      Maize|    118.71|
    +-------+--------+----------+----+---------+------+-----------+--------+-------------+------------+--------+-----------+----------+
    only showing top 5 rows
    


## 1.- Creación de un ID único
Necesitamos un código único para cada registro que sirva como clave primaria. 
Crea una nueva columna llamada Crop_ID en un nuevo DataFrame df_eng. Este ID debe seguir este formato estricto: CODIGO_REGION-CULTIVO.

**Limpieza**: de la columna Region (ej. “Region_C”), elimina la palabra “Region_” y quédate solo con la letra (puedes usar substring o split).

**Formato**: convierte el nombre del cultivo (Crop) a mayúsculas (upper).

**Concatenación**: une la letra de la región y el cultivo con un guion medio (concat_ws).

**Relleno**: si por algún motivo la letra de la región fuera muy corta (improbable aquí, pero por seguridad), asegúrate de que esa parte tenga al menos 3 caracteres rellenando con ‘X’ a la izquierda (lpad). Nota: Como en este dataset es solo una letra, el lpad rellenará con dos X, ej: “XXC”.


```python
from pyspark.sql.functions import col, upper, lpad, split, concat_ws, lit, concat

df_eng = df_cultivos.withColumn("Crop_ID",concat_ws("-",lpad(split(col("region"),"_")[1],3,"X"),upper(col("cultivo"))))

# Visualizamos el resultado para confirmar el formato CODIGO_XXC-MAIZE
df_eng.select("region", "cultivo", "Crop_ID").show(5, truncate=False)
```

    +--------+-------+----------+
    |region  |cultivo|Crop_ID   |
    +--------+-------+----------+
    |Region_C|Maize  |XXC-MAIZE |
    |Region_D|Barley |XXD-BARLEY|
    |Region_C|Rice   |XXC-RICE  |
    |Region_D|Maize  |XXD-MAIZE |
    |Region_D|Maize  |XXD-MAIZE |
    +--------+-------+----------+
    only showing top 5 rows
    


    26/01/28 12:18:15 WARN CSVHeaderChecker: CSV header does not conform to the schema.
     Header: Crop, Region
     Schema: cultivo, region
    Expected: cultivo but found: Crop
    CSV file: file:///workspace/PR0503-Limpieza_de_datos_sobre_dataset_de_cultivos/data/crop_yield_dataset.csv


## 2: Transformación matemática
Los valores de lluvia tienen mucha varianza y el rendimiento tiene demasiados decimales irrelevantes. Añade/Modifica las siguientes columnas en df_eng:

· **Log_Rainfall**: calcula el logaritmo natural (log) de la columna Rainfall_mm + 1 (para evitar errores si hubiera un 0).

· **Yield_Redondeado**: redondea el rendimiento (Yield_ton_per_ha) a 1 solo decimal usando la función round.

· **Rendimiento_Bancario**: crea otra columna usando bround sobre el rendimiento (sin decimales) para comparar cómo redondea Spark.


```python
df_eng2 = df_eng.withColumn("Log_Rainfall",F.log("lluvia_mm")+1) \
                .withColumn("Yield_Redondeado",F.round(col("ton_per_ha"),1)) \
                .withColumn("Rendimiento_Bancario",F.bround(col("ton_per_ha"),0).cast("int"))

df_eng2.select("Log_Rainfall","Yield_Redondeado","Rendimiento_Bancario").show()
```

    +------------------+----------------+--------------------+
    |      Log_Rainfall|Yield_Redondeado|Rendimiento_Bancario|
    +------------------+----------------+--------------------+
    | 8.303439375235197|           101.5|                 101|
    | 6.989963420981715|           127.4|                 127|
    |  7.88847051757027|            69.0|                  69|
    |7.9606323185829035|           169.1|                 169|
    | 7.612847161438617|           118.7|                 119|
    | 7.706250902771409|            58.9|                  59|
    | 8.213915572679692|           173.4|                 173|
    | 7.945917740076486|           170.1|                 170|
    | 7.740637555930455|           162.2|                 162|
    |6.9050893313612365|           141.7|                 142|
    | 6.344723739362192|           116.7|                 117|
    | 6.893024474294729|            95.2|                  95|
    | 8.274479558773871|            90.6|                  91|
    |  8.08447785737561|           110.6|                 111|
    |  8.28434068753965|           116.4|                 116|
    | 6.506956231659364|            77.2|                  77|
    | 7.988689896478136|           132.0|                 132|
    | 6.551407994230199|            56.9|                  57|
    | 8.103897554793402|            90.7|                  91|
    |7.5441995739511825|           143.8|                 144|
    +------------------+----------------+--------------------+
    only showing top 20 rows
    


    26/01/28 12:18:16 WARN CSVHeaderChecker: CSV header does not conform to the schema.
     Header: Rainfall_mm, Yield_ton_per_ha
     Schema: lluvia_mm, ton_per_ha
    Expected: lluvia_mm but found: Rainfall_mm
    CSV file: file:///workspace/PR0503-Limpieza_de_datos_sobre_dataset_de_cultivos/data/crop_yield_dataset.csv


## 3. Comparación de insumos
Queremos saber cuál fue el insumo químico más pesado aplicado en cada parcela. Crea una columna llamada Max_Quimico_kg.

Usa la función greatest para comparar, fila por fila, el valor de Fertilizer_Used_kg contra Pesticides_Used_kg. El resultado debe ser el valor más alto de los dos.


```python
df_insumo = df_eng2.withColumn("Max_Quimico_kg",F.greatest("abono_kg","pesticida_kg"))

df_insumo.select("abono_kg","pesticida_kg","Max_Quimico_kg").show(5)
```

    +--------+------------+--------------+
    |abono_kg|pesticida_kg|Max_Quimico_kg|
    +--------+------------+--------------+
    |   105.1|        10.2|         105.1|
    |   221.8|        35.5|         221.8|
    |    61.2|        40.0|          61.2|
    |   257.8|        42.7|         257.8|
    |   195.8|        25.5|         195.8|
    +--------+------------+--------------+
    only showing top 5 rows
    


    26/01/28 12:18:16 WARN CSVHeaderChecker: CSV header does not conform to the schema.
     Header: Fertilizer_Used_kg, Pesticides_Used_kg
     Schema: abono_kg, pesticida_kg
    Expected: abono_kg but found: Fertilizer_Used_kg
    CSV file: file:///workspace/PR0503-Limpieza_de_datos_sobre_dataset_de_cultivos/data/crop_yield_dataset.csv


## 4.- Simulación de fechas
El dataset original no tiene fecha, pero sabemos que todos estos datos corresponden a la siembra del 1 de Abril de 2023.

· Crea una columna Fecha_Siembra usando to_date sobre el literal “2023-04-01”.

· Calcula la Fecha_Estimada_Cosecha sumando 150 días a la fecha de siembra (date_add).

· Extrae el mes de la cosecha en una columna nueva llamada Mes_Cosecha (month).


```python
df_fechas = df_insumo.withColumn("Fecha_Siembra",F.to_date(F.lit("2023-4-1"))) \
                     .withColumn("Fecha_Estimada_Cosecha",F.date_add(col("Fecha_Siembra"),150)) \
                     .withColumn("Mes_Cosecha",F.month(col("Fecha_Estimada_Cosecha")))

df_fechas.select("Fecha_Siembra","Fecha_Estimada_Cosecha","Mes_Cosecha").show(5)
```

    +-------------+----------------------+-----------+
    |Fecha_Siembra|Fecha_Estimada_Cosecha|Mes_Cosecha|
    +-------------+----------------------+-----------+
    |   2023-04-01|            2023-08-29|          8|
    |   2023-04-01|            2023-08-29|          8|
    |   2023-04-01|            2023-08-29|          8|
    |   2023-04-01|            2023-08-29|          8|
    |   2023-04-01|            2023-08-29|          8|
    +-------------+----------------------+-----------+
    only showing top 5 rows
    


    26/01/28 12:18:27 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors

